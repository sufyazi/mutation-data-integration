{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Sample Names from TCGA Aliquot IDs (TCGA-BRCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# traverse a target directory and return a list of all files in it\n",
    "def get_filepaths(directory):\n",
    "    file_paths = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            file_paths.append(filepath)\n",
    "    return file_paths\n",
    "\n",
    "# get all file paths\n",
    "filepaths = get_filepaths(\"/scratch/users/ntu/suffiazi/outputs/BRCA-diff-footprinting/test\")\n",
    "\n",
    "# extract identifiers from file paths\n",
    "identifiers = [os.path.basename(path).split(\"_\")[0] for path in filepaths]\n",
    "print(identifiers)\n",
    "\n",
    "# zip identifiers and filepaths together to create a dictionary\n",
    "id_filepath_dict = dict(zip(identifiers, filepaths))\n",
    "\n",
    "# initialize empty dictionary to store column names\n",
    "id_cols_dict = {}\n",
    "\n",
    "# loop through dictionary and read in each file\n",
    "for ids, paths in id_filepath_dict.items():\n",
    "    df = pl.read_csv(paths, separator=\"\\t\")\n",
    "    cols = df.columns\n",
    "    cols = [col for col in cols if \"aliquot\" in col]\n",
    "    id_cols_dict[ids] = cols\n",
    "\n",
    "print(id_cols_dict)\n",
    "\n",
    "# save each dictionary key-value pair as one column text file\n",
    "for ids, cols in id_cols_dict.items():\n",
    "    with open(f\"/home/users/ntu/suffiazi/scripts/gatk-workflow-scripts/output_files/aliquot_ids/{ids}_aliquot_IDs.txt\", \"w\") as f:\n",
    "        for col in cols:\n",
    "            f.write(col + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# run this cell to pull uuid from aliquot ids on the GDC portal\n",
    "\n",
    "# first read in aliquot ids from text files\n",
    "\n",
    "for file in /home/users/ntu/suffiazi/scripts/gatk-workflow-scripts/output_files/aliquot_ids/*_aliquot_IDs.txt; do\n",
    "    # get dataset id from file name\n",
    "    dataset_id=$(basename \"${file}\" | cut -d \"_\" -f 1)\n",
    "    while read -r line; do\n",
    "        curl \"https://api.gdc.cancer.gov/v0/all?query=${line}&size=5\" | jq -c '.data.query.hits[].samples.hits.edges[].node.portions.hits.edges[].node.analytes.hits.edges[].node.aliquots.hits.edges[]?.node | select(.aliquot_id and .submitter_id)' | grep \"_aliquot\" >> \"/home/users/ntu/suffiazi/scripts/gatk-workflow-scripts/output_files/aliquot_id-uuid_mapping/${dataset_id}_uuids.tmp\"\n",
    "    done < \"$file\"\n",
    "    # print unique lines and remove tmp file\n",
    "    cat \"/home/users/ntu/suffiazi/scripts/gatk-workflow-scripts/output_files/aliquot_id-uuid_mapping/${dataset_id}_uuids.tmp\" | sort | uniq > \"/home/users/ntu/suffiazi/scripts/gatk-workflow-scripts/output_files/aliquot_id-uuid_mapping/${dataset_id}_uuids.txt\"\n",
    "    rm \"/home/users/ntu/suffiazi/scripts/gatk-workflow-scripts/output_files/aliquot_id-uuid_mapping/${dataset_id}_uuids.tmp\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import polars as pl\n",
    "from collections import Counter\n",
    "\n",
    "# Define a generator function to generate unique values for technical replicate renaming\n",
    "def unique_value_generator():\n",
    "    count = 1\n",
    "    while True:\n",
    "        yield count\n",
    "        count += 1\n",
    "\n",
    "# now read the uuid text files from the target path and convert to json\n",
    "directory = \"/home/users/ntu/suffiazi/scripts/gatk-workflow-scripts/output_files/aliquot_id-uuid_mapping\"\n",
    "items = os.listdir(directory)\n",
    "files = [item for item in items if os.path.isfile(os.path.join(directory, item))]\n",
    "\n",
    "for file in files:\n",
    "    # get the \n",
    "    with open(os.path.join(directory, file), \"r\") as f:\n",
    "        json_objects = [json.loads(line) for line in f]\n",
    "    # print(json_objects)\n",
    "\n",
    "    # extract dataset id from file name\n",
    "    dataset_id = file.split(\"_\")[0]\n",
    "    print(dataset_id)\n",
    "\n",
    "    # get the path to the sampsheet file of the dataset id using wildcards\n",
    "    sampsheet = glob.glob(f\"/home/users/ntu/suffiazi/scripts/atac-seq-workflow-scripts/output_files/exported_sampsheets/{dataset_id}*.csv\")\n",
    "    print(sampsheet)\n",
    "    # try loading the sampsheet as a dataframe\n",
    "    try:\n",
    "        df = pl.read_csv(sampsheet[0])\n",
    "        # print(df)\n",
    "    except:\n",
    "        print(\"Error: No sampsheet found for dataset id \" + dataset_id)\n",
    "        continue\n",
    "    # loop through the list of dictionaries and extract the uuids from the sampsheet\n",
    "    sample_ids = {}\n",
    "    for obj in json_objects:\n",
    "        print(obj[\"aliquot_id\"] + \"\\t\" + obj[\"submitter_id\"])\n",
    "        # generate a search string\n",
    "        search_string = f\"{obj['aliquot_id']}.bam\"\n",
    "        # get the SAMPLE number by searching the FILE column using the search string\n",
    "        sample_id = df.filter(df[\"FILE\"].str.contains(search_string)).select(\"SAMPLE\")\n",
    "        # squeeze the sample_id into a string using .item() method, which is the polars equivalent of pandas .squeeze() method\n",
    "        sample_id = sample_id.item()\n",
    "        # print(sample_id)\n",
    "        # print(type(sample_id))\n",
    "        # construct sample_id string for new column name\n",
    "        if sample_id < 10:\n",
    "            sample_colname = f\"{dataset_id}_sample0\" + str(sample_id)\n",
    "        else:\n",
    "            sample_colname = f\"{dataset_id}_\" + \"sample\" + str(sample_id)\n",
    "        # add the sample id to the dictionary\n",
    "        sample_ids[obj[\"submitter_id\"]] = sample_colname\n",
    "    # print(sample_ids)\n",
    "    # sort the dictionary by values\n",
    "    sample_ids = dict(sorted(sample_ids.items(), key=lambda item: item[1]))\n",
    "    print(sample_ids)\n",
    "    # create a list of values from the dictionary\n",
    "    sample_ids_list = list(sample_ids.values())\n",
    "    # check if there are any duplicate values in the list\n",
    "    if len(sample_ids_list) != len(set(sample_ids_list)):\n",
    "        # return the duplicate values and their counts\n",
    "        print(\"Warning: Duplicate values found in dictionary\")\n",
    "        count_holder = {k: v for k, v in Counter(sample_ids_list).items() if v > 1}\n",
    "        print(f\"Listing duplicated samples: {count_holder}\")\n",
    "\n",
    "        # Create an instance of the generator\n",
    "        unique_values = unique_value_generator()\n",
    "        \n",
    "        # Dictionary to keep track of replacements\n",
    "        replaced_sample_ids = {}\n",
    "\n",
    "        # Loop through the dictionary and replace duplicate values with unique values\n",
    "        for key, value in sample_ids.items():\n",
    "            if value in count_holder.keys():\n",
    "                # If the value is a duplicate, replace it with a unique value\n",
    "                sample_ids[key] = f\"{value}_0{next(unique_values)}\"\n",
    "\n",
    "        # Print the updated dictionary\n",
    "        print(sample_ids)\n",
    "    \n",
    "    # save the dictionary as tab-separated text file with each key-value pair on a new line\n",
    "    with open(f\"/home/users/ntu/suffiazi/scripts/gatk-workflow-scripts/output_files/mapping_files/{dataset_id}_sample_colname_mapping.txt\", \"w\") as f:\n",
    "        for key, value in sample_ids.items():\n",
    "            f.write(key + \"\\t\" + str(value) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Motif IDs from Bed Filename (TCGA-BRCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "while read -r line; do\n",
    "    echo $line\n",
    "    # get motif id from file name\n",
    "    motif_id=${line%_tfbs_merged_matrix-brca_brca.bed}\n",
    "    # append the motif id to a new file\n",
    "    echo $motif_id >> \"/home/users/ntu/suffiazi/scripts/gatk-workflow-scripts/input_files/tfbs_motif_prefix_list.txt\"\n",
    "done < \"/home/users/ntu/suffiazi/scripts/gatk-workflow-scripts/input_files/tfbs_bedfile_names.txt\"\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
